{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "CSE_392M.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "yrGJZiE3gN1j",
        "nqbBGOArEkfj",
        "X2nUJljGEtWX",
        "vfguAKl2SydS",
        "hMxdieN1O-g_",
        "j1I1IrmCSVKt",
        "NmXZTLwkV5DH",
        "lP2wS-_WWcJV"
      ],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "48HNJ5aPEVLj"
      },
      "source": [
        "Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l9Vt7tJeERcS"
      },
      "source": [
        "import sys \n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt \n",
        "import random \n",
        "import scipy \n",
        "import PIL\n",
        "import cv2 \n",
        "import time\n",
        "import imageio\n",
        "import scipy.ndimage\n",
        "import torch \n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "from matplotlib.pyplot import colormaps as cm \n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "from scipy import misc as ndimage\n",
        "from skfmm import distance \n",
        "from os import listdir \n",
        "from PIL import Image, ImageMath \n",
        "from torch.utils.data import TensorDataset\n",
        "from torch.utils.data import DataLoader"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tv0vG1-ME-DQ"
      },
      "source": [
        "model.py (pytorch)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "50I2-M2fFGLV"
      },
      "source": [
        "class Conv_Model(nn.Module):\n",
        "  def __init__(self, in_channels, out_channels):\n",
        "    super(Conv_Model, self).__init__()\n",
        "    self.in_channels = in_channels\n",
        "    self.out = out_channels\n",
        "    self.conv1 = nn.Conv2d(self.in_channels, self.out, kernel_size=3, stride = 1, padding=2, dilation=2) \n",
        "    self.conv2 = nn.Conv2d(self.out, self.out, kernel_size=3, stride = 1, padding=2, dilation=2)\n",
        "    self.bn1 = nn.BatchNorm2d(self.out)\n",
        "    self.bn2 = nn.BatchNorm2d(self.out)\n",
        "    self.relu = nn.LeakyReLU(0.33)\n",
        "    \n",
        "  def forward(self, x, bn_use):\n",
        "    if bn_use == True:\n",
        "      output = self.conv1(x)\n",
        "      output = self.bn1(output)\n",
        "      output = self.relu(output)\n",
        "      output = self.conv2(output)\n",
        "      output = self.bn2(output)\n",
        "      output = self.relu(output)\n",
        "    else:\n",
        "      output = self.conv1(x)\n",
        "      output = self.relu(output)\n",
        "      output = self.conv2(output)\n",
        "      output = self.relu(output)\n",
        "    return output\n",
        "\n",
        "\n",
        "class Down_Sample(nn.Module):\n",
        "  def __init__(self, in_channels, out_channels):\n",
        "    super(Down_Sample, self).__init__()\n",
        "    self.in_channels = in_channels\n",
        "    self.out_channels = out_channels\n",
        "    self.conv = Conv_Model(self.in_channels, self.out_channels)\n",
        "    self.maxpool = nn.MaxPool2d(2)\n",
        "    \n",
        "  def forward(self, x, bn_use):\n",
        "    output = self.maxpool(x)\n",
        "    output = self.conv(output, bn_use)\n",
        "    return output\n",
        "\n",
        "class Up_Sample(nn.Module):\n",
        "  def __init__(self, in_channels, out_channels):\n",
        "    super(Up_Sample, self).__init__()\n",
        "    self.in_channels = in_channels\n",
        "    self.out_channels = out_channels\n",
        "    self.up_1 = nn.ConvTranspose2d(self.in_channels, self.out_channels, kernel_size=2, stride=2) \n",
        "    self.up_model = Conv_Model(self.in_channels, self.out_channels)\n",
        "\n",
        "  def forward(self, x1, x2, bn_use):\n",
        "    x1 = self.up_1(x1)\n",
        "    x = torch.cat([x2, x1], dim=1)\n",
        "    return self.up_model(x, bn_use)\n",
        "\n",
        "\n",
        "class model(nn.Module):\n",
        "  def __init__(self, channels, device = torch.device(\"cuda\")):\n",
        "    super(model, self).__init__()\n",
        "    self.in_channels = channels\n",
        "    self.stride = 1\n",
        "    self.out = 4\n",
        "    self.device = device\n",
        "\n",
        "    self.conv_model = Conv_Model(self.in_channels, self.out)\n",
        "    self.down1 = Down_Sample(self.out, 2*self.out)\n",
        "    self.down2 = Down_Sample(2*self.out, 4*self.out)\n",
        "    self.down3 = Down_Sample(4*self.out, 8*self.out)\n",
        "    self.down4 = Down_Sample(8*self.out, 16*self.out)\n",
        "    self.down5 = Down_Sample(16*self.out, 32*self.out)\n",
        "    self.down6 = Down_Sample(32*self.out, 64*self.out)\n",
        "    self.up5 = Up_Sample(64*self.out, 32*self.out)\n",
        "    self.up4 = Up_Sample(32*self.out, 16*self.out)\n",
        "    self.up3 = Up_Sample(16*self.out, 8*self.out)\n",
        "    self.up2 = Up_Sample(8*self.out, 4*self.out)\n",
        "    self.up1 = Up_Sample(4*self.out, 2*self.out)\n",
        "    self.up0 = Up_Sample(2*self.out, self.out)\n",
        "    self.output = nn.Conv2d(self.out, 1, kernel_size = 1)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x1 = self.conv_model(x, True) \n",
        "    x1_down = self.down1(x1, True) \n",
        "    x2_down = self.down2(x1_down, True) \n",
        "    x3_down = self.down3(x2_down, True) \n",
        "    x4_down = self.down4(x3_down, True) \n",
        "    x5_down = self.down5(x4_down, False)\n",
        "    x6_down = self.down6(x5_down, False) \n",
        "    x5_up = self.up5(x6_down, x5_down, True)\n",
        "    x4_up = self.up4(x5_up, x4_down, True)\n",
        "    x3_up = self.up3(x4_up, x3_down, True) \n",
        "    x2_up = self.up2(x3_up, x2_down, True) \n",
        "    x1_up = self.up1(x2_up, x1_down, True) \n",
        "    x0_up = self.up0(x1_up, x1, True) \n",
        "    logits = self.output(x0_up)\n",
        "    sig = nn.Sigmoid()\n",
        "    output = sig(logits)\n",
        "    return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q2tdLURATdLS"
      },
      "source": [
        "train.py (pytorch)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bGNWqv22Tg_W"
      },
      "source": [
        "def train(train_input, train_label, device=torch.device(\"cuda\")):\n",
        "  base_dir = '/content/drive/MyDrive/Colab_Notebooks'\n",
        "  train_label.requires_grad=False\n",
        "\n",
        "  # 1 channel for frontiers and 1 for cumulative visibility\n",
        "  train_model  = model(channels=2, device = device).double().to(device) \n",
        "  optimizer = optim.Adam(train_model.parameters(), lr = 1e-3)\n",
        "\n",
        "  # Schedule decrease in learning rate every 200 epochs\n",
        "  scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones = [200,400,600,800,1000], gamma = 0.1)\n",
        "\n",
        "  loss_fn = nn.BCELoss()\n",
        "  loss_fn_eval = nn.BCELoss()\n",
        "  loss_list = []\n",
        "  loss_list_eval = []\n",
        "  num_epochs = 10000\n",
        "  batchsize = 25\n",
        "  data = TensorDataset(train_input, train_label)\n",
        "  full_size = len(train_label)\n",
        "  train_size = int(0.9*full_size)\n",
        "  test_size = full_size - train_size\n",
        "  train_dataset, test_dataset = torch.utils.data.random_split(data, [train_size, test_size])\n",
        "  train_loader = DataLoader(dataset=train_dataset, batch_size = batchsize, shuffle = True)\n",
        "  test_loader = DataLoader(dataset=test_dataset, batch_size=batchsize, shuffle=False)\n",
        "\n",
        "  # Feed through all the target data\n",
        "  for epoch in range(num_epochs):\n",
        "    running_loss = 0.0\n",
        "    running_loss_eval = 0.0\n",
        "    counter = 0\n",
        "    train_model.train() # Put model in training mode\n",
        "    start = time.time()\n",
        "    for xbatch, ybatch in train_loader:\n",
        "      xbatch = xbatch.to(device)\n",
        "      ybatch = ybatch.to(device)\n",
        "      optimizer.zero_grad()\n",
        "      output = train_model(xbatch).squeeze()\n",
        "      loss = loss_fn(output, ybatch)\n",
        "      loss.backward(retain_graph=True)\n",
        "      \n",
        "      #if (epoch < 25 or epoch > 0.5*num_epochs) and counter%999==0:\n",
        "      if counter%999==0:\n",
        "        print('expected training gain is:')\n",
        "        plt.imshow(ybatch[0].cpu().detach().numpy()); plt.show(); plt.close()\n",
        "        print('output training gain is:')\n",
        "        plt.imshow(output[0].cpu().detach().numpy()); plt.show(); plt.close()\n",
        "        print('LOSS IS:', loss)\n",
        "        print('Iteration {} completed'.format(counter+1))\n",
        "        print()\n",
        "\n",
        "      optimizer.step()\n",
        "      scheduler.step()\n",
        "      counter+=1\n",
        "      running_loss += loss.item()\n",
        "      torch.cuda.empty_cache\n",
        "      counter += 1\n",
        "    \n",
        "    # Validate model after each epoch\n",
        "    train_model.eval()\n",
        "    with torch.no_grad():\n",
        "      for xbatch_eval, ybatch_eval in test_loader:\n",
        "        xbatch_eval = xbatch_eval.to(device)\n",
        "        ybatch_eval = ybatch_eval.to(device)\n",
        "        output_eval = train_model(xbatch_eval).squeeze()\n",
        "        loss_eval = loss_fn_eval(output_eval, ybatch_eval)\n",
        "        running_loss_eval += loss_eval.item()\n",
        "        torch.cuda.empty_cache\n",
        "      \n",
        "    end = time.time()\n",
        "    total_time = end-start\n",
        "    minutes = total_time//60\n",
        "    seconds = total_time%60\n",
        "    running_loss = running_loss/train_size\n",
        "    running_loss_eval = running_loss_eval/test_size\n",
        "    loss_list.append(running_loss)\n",
        "    loss_list_eval.append(running_loss_eval)\n",
        "    \n",
        "    print('Training loss is')\n",
        "    plt.plot(loss_list); plt.show(); plt.close()\n",
        "    print('Validation loss is')\n",
        "    plt.plot(loss_list_eval); plt.show(); plt.close()\n",
        "    \n",
        "    print('Epoch {}/{}'.format(epoch+1, num_epochs))\n",
        "    print('Running training loss is:', running_loss)\n",
        "    print('Running validation loss is:', running_loss_eval)\n",
        "    \n",
        "    if (epoch+1)%10==0:\n",
        "      torch.save(train_model.state_dict(), base_dir + \"/Models3/bce_BN_model_{}.pt\".format(epoch+1))\n",
        "      torch.save(loss_list, base_dir + \"/Losses3/bce_BN_loss_{}.pt\".format(epoch+1))\n",
        "      torch.save(loss_list_eval, base_dir + \"/Losses3/bce_BN_validation_loss_{}.pt\".format(epoch+1))\n",
        "    print('Time is:', minutes,'minutes and', seconds, 'seconds')\n",
        "    print()\n",
        "\n",
        "  return train_model, loss_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vl12nQedE2Uh"
      },
      "source": [
        "Main"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "voZ_93WZMlXf"
      },
      "source": [
        "# Load the data\n",
        "\n",
        "train_data= load_data()\n",
        "train_imgs, train_labels =  zip(*train_data)\n",
        "train_labels = list(train_labels)\n",
        "\n",
        "states = torch.stack(train_imgs)\n",
        "target = torch.stack(train_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eZb29zYZD3id"
      },
      "source": [
        "def main():\n",
        "  device=torch.device(\"cuda\")\n",
        "  base_dir = '/content/drive/MyDrive/Colab_Notebooks'\n",
        "\n",
        "  model, loss = train(states, target, device = device)\n",
        "  torch.save(model.state_dict(), base_dir + \"/model_bce_final.pt\")\n",
        "  \n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jZgVmPFVxzoq"
      },
      "source": [
        "Load Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_7-WQYkRxKQy"
      },
      "source": [
        "import torch\n",
        "device=torch.device(\"cuda\")\n",
        "\n",
        "def load_data():\n",
        "  base_dir = '/content/drive/MyDrive/Colab_Notebooks'\n",
        "  train_data = torch.load(base_dir + '/train_data_filtered.pt')\n",
        "  return train_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yrGJZiE3gN1j"
      },
      "source": [
        "Generate Training Examples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aw2_eIR8gL0c"
      },
      "source": [
        "def generate_training():\n",
        "  # Get cumulative visibility and frontiers for input\n",
        "\n",
        "  #seed_everything() # Seed to get reproducible results\n",
        "  for i in range(1):\n",
        "    print('current iteration is:', i)\n",
        "    run_sequence(i)\n",
        "\n",
        "generate_training()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nqbBGOArEkfj"
      },
      "source": [
        "Greedy Algorithm Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z7KgYwYFEiL3"
      },
      "source": [
        "def vis2d(phi, O):\n",
        "# phi is the sdf of the scene\n",
        "# O is the grid index of the vantage point\n",
        "\n",
        "    psi = 1*phi\n",
        "\n",
        "    # streamlined sweeping so we don't have to think about the indices\n",
        "    for i in range(2):\n",
        "        si = i*1 + i-1   # gives {-1 or 1}\n",
        "        for j in range(2):\n",
        "            sj = j*1 + j-1\n",
        "            psi = sweep2d(phi, psi, O, si, sj)\n",
        "\n",
        "    return psi\n",
        "\n",
        "\n",
        "def compute_max_index(N, s, O):\n",
        "# determine how many steps to take based on if sweeping forward or backwards\n",
        "    if s == 1:\n",
        "        indMax = N-O\n",
        "    elif s == -1:\n",
        "        indMax = O -(-1)\n",
        "    return indMax\n",
        "\n",
        "\n",
        "def sweep2d(phi, psi, O, si, sj):\n",
        "# a way to write the sweep so that it is modular\n",
        "# basically propagates information from the origin.\n",
        "# si,sj,sk determine the direction of the indices and the sign of r\n",
        "\n",
        "# need to turn off boundscheck for faster\n",
        "\n",
        "    Ny = phi.shape[0]\n",
        "    Nx = phi.shape[1]\n",
        "\n",
        "    # compute indices\n",
        "    I = compute_max_index(Ny, si, O[0])\n",
        "    J = compute_max_index(Nx, sj, O[1])\n",
        "\n",
        "    # a weird way to index the loop so that it can use c compatible range function\n",
        "    for ri in range(I):\n",
        "        i = O[0] + si*ri\n",
        "        for rj in range(J):\n",
        "            j = O[1] + sj*rj\n",
        "            if (ri+rj !=0 ):  # this if should be removed but doesn't seem to affect much\n",
        "                A = 1.0/(ri+rj)\n",
        "\n",
        "                # clamp indices between 0 and N. The edges shouldn't matter because ri, rj would be zero if they fall off\n",
        "                i_ = max(0, min(Ny-1, (i-si) ) )\n",
        "                j_ = max(0, min(Nx-1, (j-sj) ) )\n",
        "                psi[i,j] = A*(ri*psi[i_,j]+rj*psi[i,j_])\n",
        "                psi[i,j] = min(phi[i,j],psi[i,j])\n",
        "\n",
        "    return psi\n",
        "\n",
        "def delta(x, epsilon):\n",
        "    chi = (x>(-epsilon/2)) * (x<(epsilon/2))\n",
        "    y = 2/epsilon * chi * np.cos(np.pi * x / epsilon) **2\n",
        "    return y\n",
        "\n",
        "def im2double(img): # Normalize array\n",
        "    min_val = np.min(1*img.ravel())\n",
        "    max_val = np.max(1*img.ravel())\n",
        "    out = (img.astype('float') - min_val) / (max_val - min_val)\n",
        "    return out\n",
        "\n",
        "def plot_pos(phi, x0_one = None, x0_two = None, h_1 = None, x0 = None):\n",
        "  # Plot positive part (usually for cumulative visibility and frontiers)\n",
        "\tplt.imshow(phi>0, cmap = 'gray')\n",
        "\tif h_1 != None:\n",
        "\t\tplt.plot(x0_one, x0_two, 'ro') \n",
        "\t\tfor i in range (1,h_1):\n",
        "\t\t\tplt.plot((x0[i])[1], (x0[i])[0], 'bo') #; plt.plot(x1[1], x1[0], 'ro'); \t\t\n",
        "\tif x0_one != None and x0_two != None:\n",
        "\t\tplt.plot(x0_one, x0_two, 'ro')\n",
        "\tplt.show(); plt.close()\n",
        "\n",
        "def initialize_point(img, dx): \n",
        "\t# Initialize point in free space--so where ever there is white space (255) or 1 when normalized\n",
        "\tfree_space = False\n",
        "\twhile free_space == False:\n",
        "\t\tx_pos = random.randint(0, img.shape[0]-1)\n",
        "\t\ty_pos = random.randint(0, img.shape[1]-1)\n",
        "\treturn [x_pos*dx, y_pos*dx]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WzS4hs5xT_9O"
      },
      "source": [
        "def seed_everything(seed=1):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X2nUJljGEtWX"
      },
      "source": [
        "Greedy Algorithm"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AH3YHWxWEyfR"
      },
      "source": [
        "import numpy.random as random\n",
        "from skfmm import distance\n",
        "\n",
        "def setup(h, w):\n",
        "  # SET UP A GRID OVER [-1,1]x[-1,1]\n",
        "  m = h\n",
        "  dx = 2/m\n",
        "  grid_space = np.linspace(-1,1-dx,m)\n",
        "  [x,y] = np.meshgrid(grid_space, grid_space) # 2D Array (list)\n",
        "\n",
        "  # CREATE LEVEL SET FUNCTION\n",
        "  num_circles = random.randint(2,6)\n",
        "  phi = np.ndarray(shape=(h,w), dtype=float)\n",
        "  x_vals = [] # x_vals and y_vals are the centers of the circles\n",
        "  y_vals = []\n",
        "  radii = []\n",
        "  for i in range(num_circles):\n",
        "    x_val = random.uniform(-0.8,0.8)\n",
        "    y_val = random.uniform(-0.8,0.8)\n",
        "    rad_val = random.uniform(0.12,0.46) \n",
        "    x_vals.append(x_val)\n",
        "    y_vals.append(y_val)\n",
        "    radii.append(rad_val)\n",
        "\n",
        "  norm_factor = 0\n",
        "  for i in range(h):\n",
        "    for j in range(w):\n",
        "        phi_vals = []\n",
        "        for k in range(num_circles):\n",
        "          phi_temp = np.sqrt((x[i][j]-x_vals[k])**2 + (y[i][j]-y_vals[k])**2)- radii[k]\n",
        "          phi_vals.append(phi_temp)\n",
        "          phi[i,j] = min(phi_vals)\n",
        "        if phi[i,j] > 0:\n",
        "          norm_factor += 1\n",
        "  return phi, norm_factor\n",
        "\n",
        "def initial_point(h, w, phi):\n",
        "  # SET x0 AND FIND VISIBILITY FOR x0; choose initial point x0 that is not in a black region\n",
        "  x_init = random.randint(1, w-2)\n",
        "  y_init = random.randint(1, h-2)\n",
        "\n",
        "  in_obstacle = True\n",
        "  while in_obstacle:\n",
        "    if phi[x_init, y_init] <= 0:\n",
        "      x_init = random.randint(1, w-2)\n",
        "      y_init = random.randint(1, h-2)\n",
        "    else:\n",
        "      in_obstacle = False\n",
        "      break\n",
        "\n",
        "  x0 = np.array([[x_init, y_init]])\n",
        "  return x0\n",
        "\n",
        "def calc_frontiers(h, w, phi, psi_cum, dx):\n",
        "  return delta(psi_cum,2*dx)*dx*(delta(phi,2*dx)==0)\n",
        "\n",
        "def compute_gain(phi, psi_cum, x0, gain_tol, norm_factor, h, w, finished):\n",
        "  # Index with highest gain is the next vantage point\n",
        "  gain_max = 0 # To keep track of maximum gain across all pixels\n",
        "  gain_x0 = np.zeros(shape=(h,w), dtype=float) # Gain is h*w matrix with gain for each potential new vantage pt\n",
        "  psi_temp = np.ndarray(shape=(h,w), dtype=float) # This will contain total visibility for each pixel\n",
        "\n",
        "  for i in range (h):\n",
        "    for j in range (w): # How much you can see at each point--max of this should be next point\n",
        "      if phi[i,j]>0: # Only do computation for points not in the obstacles\n",
        "        psi_temp = np.array(vis2d(phi, np.array([i,j]))) # Total visibility of point x_i\n",
        "\n",
        "        # Compute gain function at potential vantage point z\n",
        "        for k in range (h):\n",
        "          for l in range (w):\n",
        "            '''\n",
        "            psi_cum[k,l] < 0 means we couldn't see this point before from our collection of vantage points\n",
        "            psi_temp[k,l] > 0 means we can currently see this point from candidate vantage point [i,j]\n",
        "            phi[k,l] > 0 means it's not inside an obstacle\n",
        "            '''\n",
        "            if (psi_cum[k,l]<0) and (psi_temp[k,l]>0) and (phi[k,l]>0): # <=0 includes boundaries\n",
        "              gain_x0[i,j] += 1 # The gain is one more grid square/pixel\n",
        "\n",
        "  # Find the point with maximum gain to be the next candidate point\n",
        "  x_max = -1\n",
        "  y_max = -1\n",
        "  for i in range (h):\n",
        "    for j in range (w):\n",
        "      if (gain_x0[i,j] > gain_max) and ([i,j] not in x0):\n",
        "        gain_max = gain_x0[i,j]\n",
        "        x_max = i\n",
        "        y_max = j\n",
        "  if (gain_max/norm_factor)<gain_tol: # Stop when gains are no longer significant\n",
        "    print('no more gains')\n",
        "    finished = True\n",
        "  x1 = np.array([[x_max, y_max]])\n",
        "  x0 = np.append(x0, x1, axis=0) # This is an array containing all vantage points\n",
        "  print('gain max:', gain_max)\n",
        "  print('the chosen vantage point is: (' + str(x_max) + ',' + str(y_max) +')')\n",
        "\n",
        "  gain_x0 = gain_x0/norm_factor\n",
        "  return gain_x0, x0, finished\n",
        "\n",
        "def greedy_alg(psi_cum, phi, psi_0, x0, frontiers, h, w, iteration_counter, norm_factor):\n",
        "  # FIND THE OTHER POINTS/FINISH THE GREEDY ALGORITHM\n",
        "  base_dir = '/content/drive/MyDrive/Colab_Notebooks'\n",
        "  counter = 0 # Counter is used to save the cumulative visibility and frontiers and gains under the same number/label\n",
        "  finished = False\n",
        "  dx = 2/h\n",
        "  gain_tol = 1e-5\n",
        "  psi_1 = psi_0\n",
        "\n",
        "  while not finished:\n",
        "    # Save the cumulative visibility and frontiers\n",
        "    #torch.save(psi_cum, base_dir + \"/cumulative_visibility/cumulative_visibility_{}_{}.pt\".format(iteration_counter, counter))\n",
        "    #torch.save(frontiers, base_dir + \"/frontiers/frontiers_{}_{}.pt\".format(iteration_counter, counter))\n",
        "\n",
        "    # Compute gain function and the next vantage point\n",
        "    gain_x0, x0, finished = compute_gain(phi, psi_cum, x0, gain_tol, norm_factor, h, w, finished)\n",
        "    plt.imshow(gain_x0); plt.show(); plt.close()\n",
        "    #torch.save(gain_x0, base_dir + \"/gains/gain_{}_{}.pt\".format(iteration_counter, counter))\n",
        "\n",
        "    # PLOT THE POINTS WITH CURRENT VANTAGE POINT\n",
        "    h_1, m_1= x0.shape\n",
        "    psi_1 = np.array(vis2d(phi, x0[-1])) # Compute new visibility\n",
        "    print('this is x{}'.format(counter))\n",
        "    #plot_pos(psi_1, x0_one = (x0[0])[1], x0_two = (x0[0])[0], h_1 = h_1, x0 = x0)\n",
        "\n",
        "    # Calculate new cumulative visibility\n",
        "    psi_1 = distance(psi_1,dx)\n",
        "    psi_cum = np.maximum(psi_cum, psi_1)\n",
        "    frontiers = calc_frontiers(h, w, phi, psi_cum, dx) # Compute new frontiers\n",
        "\n",
        "    # Plot cumulative visibility and frontiers\n",
        "    plot_pos(psi_cum, x0_one = (x0[0])[1], x0_two = (x0[0])[0], h_1 = h_1, x0 = x0)\n",
        "    plot_pos(frontiers)\n",
        "\n",
        "    # PLOT THE FRONTIERS (INTERSECTIONS OF WHAT CAN BE SEEN AND WHAT COULD BE SEEN)\n",
        "    # plt.plot()\n",
        "    counter += 1\n",
        "\n",
        "\n",
        "def run_sequence(iteration_counter, output_path = None):\n",
        "# demonstration of how to setup the scene and observing locations to use the visibility algorithm to find the path \n",
        "# psi is visibility\n",
        "\n",
        "  device=torch.device(\"cuda\")\n",
        "  # SET UP VARIABLES\n",
        "  h,w = 64,64\n",
        "  m = h\n",
        "  dx = 2/m \n",
        "  tol = 1e-2\n",
        "  eps = 2*dx\n",
        "\n",
        "  # Calculate initial cumulative visibility from initial point\n",
        "  psi_cum = -100+np.zeros(shape=(h,w), dtype=float) # Cumulative visibility\n",
        "  phi, norm_factor = setup(h, w)\n",
        "  x0 = initial_point(h, w, phi)\n",
        "\n",
        "  psi_0 = np.array(vis2d(phi, x0[0])) # This is the visibility from x0\n",
        "  psi_0 = distance(psi_0,dx) # distance is a function from skfmm (sci kit fast marching method)\n",
        "  psi_cum = np.maximum(psi_cum, psi_0) \n",
        "  \n",
        "  frontiers = calc_frontiers(h, w, phi, psi_cum, dx)\n",
        "\n",
        "  # PLOT PSI_0 AND PSI_CUM AND FRONTIERS\n",
        "  print('the environment is')\n",
        "  plot_pos(phi)\n",
        "  print('this is x0')\n",
        "  print('psi_0:')\n",
        "  plot_pos(psi_0, (x0[0])[1], (x0[0])[0])\n",
        "  print('initial cumulative visibility')\n",
        "  plot_pos(psi_cum, (x0[0])[1], (x0[0])[0])\n",
        "  print('frontiers')\n",
        "  plot_pos(frontiers)\n",
        "\n",
        "  '''\n",
        "  iteration_counter is the labeled number of the environment\n",
        "  example: iteration_counter = 3000 means this is the 3000th map\n",
        "  '''\n",
        "  greedy_alg(psi_cum, phi, psi_0, x0, frontiers, h, w, iteration_counter, norm_factor)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}